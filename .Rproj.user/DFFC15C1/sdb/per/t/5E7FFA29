{
    "collab_server" : "",
    "contents" : "library(\"dplyr\")\t\t# tibble function\n\n\nqvalue2 <- function(p, alpha=NULL, lam=NULL, robust=F)\n{\n#This is a function for estimating the q-values for a given set of p-values. The\n#methodology comes from a series of recent papers on false discovery rates by John\n#D. Storey et al. See http://www.stat.berkeley.edu/~storey/ for references to these\n#papers. This function was written by John D. Storey. Copyright 2002 by John D. Storey.\n#All rights are reserved and no responsibility is assumed for mistakes in or caused by\n#the program.\n#\n#Input\n#=============================================================================\n#p: a vector of p-values (only necessary input)\n#alpha: a level at which to control the FDR (optional)\n#lam: the value of the tuning parameter to estimate pi0 (optional)\n#robust: an indicator of whether it is desired to make the estimate more robust\n#        for small p-values (optional)\n#\n#Output\n#=============================================================================\n#remarks: tells the user what options were used, and gives any relevant warnings\n#pi0: an estimate of the proportion of null p-values\n#qvalues: a vector of the estimated q-values (the main quantity of interest)\n#pvalues: a vector of the original p-values\n#significant: if alpha is specified, and indicator of whether the q-value fell below alpha\n#    (taking all such q-values to be significant controls FDR at level alpha)\n\n#This is just some pre-processing\n    m <- length(p)\n#These next few functions are the various ways to automatically choose lam\n#and estimate pi0\n    if(!is.null(lam)) {\n        pi0 <- mean(p>lam)/(1-lam)\n        remark <- \"The user prespecified lam in the calculation of pi0.\"\n    }\n    else{\n        remark <- \"A smoothing method was used in the calculation of pi0.\"\n        library(stats)\n        lam <- seq(0,0.95,0.01)\n        pi0 <- rep(0,length(lam))\n        for(i in 1:length(lam)) {\n        pi0[i] <- mean(p>lam[i])/(1-lam[i])\n        }\n        spi0 <- smooth.spline(lam,pi0,df=3,w=(1-lam))\n        pi0 <- predict(spi0,x=0.95)$y\n    }\n#The q-values are actually calculated here\n    u <- order(p)\n    v <- rank(p)\n    qvalue <- pi0*m*p/v\n    if(robust) {\n        qvalue <- pi0*m*p/(v*(1-(1-p)^m))\n        remark <- c(remark, \"The robust version of the q-value was calculated. See Storey JD (2002) JRSS-B 64: 479-498.\")\n    }\n    qvalue[u[m]] <- min(qvalue[u[m]],1)\n    for(i in (m-1):1) {\n    qvalue[u[i]] <- min(qvalue[u[i]],qvalue[u[i+1]],1)\n    }\n#Here the results are returned\n    if(!is.null(alpha)) {\n        return(remarks=remark, pi0=pi0, qvalue=qvalue, significant=(qvalue <= alpha), pvalue=p)\n    }\n    else {\n        return(list(remarks=remark, pi0=pi0, qvalue=qvalue, pvalue=p))\n    }\n}\n\n# installing packages from archieves (may need Rtools)\n#------------------------------------------------------\ninstall.packages(devtools)\nrequire(devtools)\n\nqvalue <- \"https://cran.r-project.org/src/contrib/Archive/qvalue/qvalue_1.26.0.tar.gz\"\ninstall.packages(qvalue, repos=NULL, type=\"source\")\n\nreshape <- \"http://cran.r-project.org/src/contrib/Archive/reshape/reshape_0.8.4.tar.gz\"\ninstall_url(reshape)\ninstall.packages(\"qvalue\")\nlibrary(reshape)\nlibrary(qvalue)\ninstall.packages(Rtools)\n\nurl <- 'http://cran.r-project.org/src/contrib/Archive/doRedis/doRedis_1.0.5.tar.gz'\ninstall_url(url)\n\n\n\n# binary case---------------------\n#library(MASS)\nlibrary(qvalue)\n\n\n#rm(list=ls())\nlibrary(mvnfast)\t\t# fast generate multi variate normal\n#source(\"https://bioconductor.org/biocLite.R\")\n#biocLite(\"IHW\")\nlibrary(\"IHW\")\t\t# independent hypotheis weight\n\n\n##############################################################################################\n#----------------Function-0: RoederWasermanWeight---------------------------------------------\n# Function to estiamte weight from data by using Roeder-Waserman algorithm\n# testStat = test statistics,\n# m = number of test statistics\n# gamma = smooting parameter\n# alpha = Significance level\n# rk = k-th group size\n#---------------------------------------------------------------------------------------------\n\nRoederWasermanWeight <- function(testStat,m,gamma,alpha,rk=1000)\n\t{\n\tk <- m/rk\n\ttestGroup <- rep(1:k,each=rk)\t\t\t\t # Step-1: patitioning 'm' tests into groups\n\ttestMeans <- tapply(testStat,testGroup,mean) \t #Step-2: calculate mean and stdv for each group\n\ttestSd <- tapply(testStat,testGroup,sd)\n\tpi_hat <- testMeans^2/(testMeans^2+testSd^2-1) \t\t\t#Step-4: estiamte pi and effect size\n\teffect_hat <- testMeans/pi_hat\n\teffect_hat[pi_hat <= 1/rk] <- 0\n\n\tif(sum(effect_hat,na.rm=T)==0) normWeight.w <- rep(1,m) else{\n     \ttheta = as.vector(effect_hat)\n     \tc0 = (qnorm(1-alpha/(2*m)))^2/2\n\tfindc = function(thet,alpha,c0,m) \t\t\t# finding constant c\n\t\t{\n\t\tcc = seq(.1,c0,.005)\n\t\ttot <- sapply(cc,function(c) return(sum((m/alpha)*(1-pnorm(thet/2 + c/thet)))))\n\t\tcout = cc[min(abs(tot-m))==abs(tot-m)]\n\t\tcoutAdj = ifelse(length(cout)>1,sample(cout,1),cout)\n\t\treturn(coutAdj)\n\t\t}\n\tc <- sapply(theta,findc,alpha,c0,m)\n\tweight_k <- (m/alpha)*(1-pnorm(effect_hat/2 + c/effect_hat))  \t# Step-5: calculate weight\n\tweight_k_smooth <- (1-gamma)*weight_k + gamma*sum(weight_k)/k  \t# smoothing weight by smotthing parameter gamma\n\tweight_tests <- rep(weight_k_smooth, each=rk)\t\t\t\t# distribute weight over all tests\n\tnormWeight.w <- weight_tests/sum(weight_tests)*m}\t\t\t# little adjustment to obtain sum of weight=m\n\treturn(normWeight.w)\n\t}\n\n#RoederWasermanWeight(testStat=OD$tt,m=m,gamma=.05,alpha=alpha,rk=1000)\n\n\n\n\n\n\n\n\n\n\n#par(mfrow=c(4,3))\n\n# manual method\n# binary case\n#=========================-\nm <- 100000\nm0 <- m*.5\nm1 <- m-m0\nset.seed(123)\npval0 <- runif(m0,0,1)\nset.seed(123)\npval1 <-rbeta(m1,1,10)\npvalue <- c(pval0,pval1)\nmodel <- c(rep(0,m0),rep(1,m1))\npvalueModel <- cbind(pvalue,model)\ncolnames(pvalueModel) <- c(\"pval\",\"model\")\nOrdered.pvalue <- pvalueModel[order(pvalueModel[,1]),]\n\ngrp <- 100\ngrpSize <- 1000\npi0 <- c()\npi1 <- c()\nfor(i in 1:grp)\n{\npi0[i] <- sum(Ordered.pvalue[(i*grpSize-grpSize+1):(i*grpSize) ,2]==0)/grpSize\npi1[i] <- sum(Ordered.pvalue[(i*grpSize-grpSize+1):(i*grpSize) ,2]==1)/grpSize\n}\nmatplot(1:grp,cbind(pi0,pi1))\n\n\n# manual method\n# binary case ordered by filter staistics\n#==========================================\nm <- 100000\nnull=.5\nm0 <- m*null\nm1 <- m-m0\neffectVec <- seq(0,5,.5)\ni=5\nH <- rbinom(m,1,1-null)          \t   # alternative hypothesis true or false\nef <- effectVec[i]\ntest.ef <- rnorm(m,ef*H,1)\ntest.et <- rnorm(m,ef*H,1)\npval.ef <- 1-pnorm(test.ef)\npval.et <- 1-pnorm(test.et)\nData <- data.frame(test.ef,pval.ef,test.et,pval.et,H)\nhist(pval.et,xlab=\"pvalue\",ylab=\"frequency\",main=\"distribution of pvalues\")\nordData <- Data[order(test.ef,decreasing=T),]\nOrdered.pvalue <- ordData[,4:5]\nOrdered.pvalue <- ordData[,4]\n\ngrp <- 100\ngrpSize <- 1000\npi0 <- c()\npi1 <- c()\nfor(i in 1:grp)\n{\npi0[i] <- sum(Ordered.pvalue[(i*grpSize-grpSize+1):(i*grpSize) ,2]==0)/grpSize\npi1[i] <- sum(Ordered.pvalue[(i*grpSize-grpSize+1):(i*grpSize) ,2]==1)/grpSize\n}\nmatplot(1:grp,cbind(pi0,pi1),type=\"l\",lwd=2,xlab=\"ranks\",ylab=\"P(rank|effect=2)\")\nlegend(\"top\",c(\"null model\",\"alt. model\"),lty=1:2,col=1:2,lwd=2)\n\n# right way to obtain probbaility for the binary case\n#--------------------------------------------\nProb <- c()\nfor(j in 1:grp)\n\t{\n\tp = Ordered.pvalue[(j*grpSize-grpSize+1):(j*grpSize)]\n\tProb[j] <- 1-qvalue(p=p,pi0.method=\"bootstrap\",lambda=max(p))$pi0\n\t}\nProb = Prob/sum(Prob)\t# normalized\nplot(Prob,type=\"l\")\n\nx=1:100\ny=Prob\nmod = lm(y~ns(x,df=3))\nplot(mod$fit,type=\"l\")\n\n\n# continuous case1 (filter pvalue = effect pvalue)\n#================================================\nranksProbInfo <- array(NA,c(10,100,2))\nfor(j in 1:100)\n{\npvalPerGrp <- Ordered.pvalue[(j*grpSize-grpSize+1):(j*grpSize) ,1]\n#h <- hist(pvalPerGrp,freq = FALSE,breaks=seq(min(pvalPerGrp),max(pvalPerGrp),length=11))\nh <- hist(pvalPerGrp,freq = FALSE,breaks=seq(0,1,length=11))\n#h$counts=h$counts/sum(h$counts)\n#plot(h)\nh$density=h$density/sum(h$density)\nxaxisValue <- h$mids\t\t\t\t# pvalue or quantile relates to effect size\nyaxisValue <- h$density/sum(h$density)\t# density or probability P(effect|rank)\nranksProbInfo[,j,1] <- xaxisValue\nranksProbInfo[,j,2] <- yaxisValue\n}\n\n#par(mfrow=c(3,3))\nfor(i in 1:9)\n{\n#plot(1:100,ranksProbInfo[i,,1])\n#sum(ranksProbInfo[i,,1])\n\nplot(1:100,ranksProbInfo[i,,2],type=\"l\",lwd=2)\nsum(ranksProbInfo[i,,2])\n}\n\n\npvalue <- c(0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95)\neffectsize <- qnorm(pvalue)\n\n\n\n\n\n# continuous case2 (filter pvalue != effect pvalue)\n#================================================\nm <- 100000\nm0 <- m*.5\nm1 <- m-m0\np.filter <- c(runif(m0),rbeta(m1,1,10))\np.test <- c(runif(m0),rbeta(m1,1,10))\nOrdered.pvalue2 <- p.test[order(p.filter)]\n\n\ngrp <- 100\ngrpSize <- 1000\nranksProbInfo <- array(NA,c(10,100,2))\nfor(j in 1:100)\n{\npvalPerGrp <- Ordered.pvalue2[(j*grpSize-grpSize+1):(j*grpSize)]\n#h <- hist(pvalPerGrp,freq = FALSE,breaks=seq(min(pvalPerGrp),max(pvalPerGrp),length=11))\nh <- hist(pvalPerGrp,freq = FALSE,breaks=seq(0,1,length=11))\n#h$counts=h$counts/sum(h$counts)\n#plot(h)\nh$density=h$density/sum(h$density)\nxaxisValue <- h$mids\t\t\t\t# pvalue or quantile relates to effect size\nyaxisValue <- h$density/sum(h$density)\t# density or probability P(effect|rank)\nranksProbInfo[,j,1] <- xaxisValue\nranksProbInfo[,j,2] <- yaxisValue\n}\n\npar(mfrow=c(3,3))\nfor(i in 1:9)\n{\n#plot(1:100,ranksProbInfo[i,,1])\n#sum(ranksProbInfo[i,,1])\n\nplot(1:100,ranksProbInfo[i,,2],type=\"l\",lwd=2)\nsum(ranksProbInfo[i,,2])\n}\n\npvalue <- c(0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95)\neffectsize <- qnorm(1-pvalue)\n\nweight_rejection <- c()\nbon_rejection <- c()\nfor(k in 1:10)\n{\ney <- effectsize[k]\nrankProbsH01 <- ranksProbInfo[k,,2]\nrankProbsH01_norm <- rankProbsH01/sum(rankProbsH01_norm)\n\n# function to compute normalized weight for the binary case\n\t# prob=P(rank=k|filterEffect=ey), ey=filter efffect (effect of Y)\n\t#-----------------------------------------------------------------------------------------------------------\n\tfun.nomWeight.pro.bin <- function(prob,ey,alpha)\n\t\t{\t\t\t\t\t\t\t#input:p(rank|effect),filter effect; output: normalized weight\n\t\tm = length(prob)\n\t\tdelta <- seq(0,1,length=10000)\t\t# lagrange multiplier/ tuning parameter to obtain normalized wieght\n\t\tfindDelta <- function(delta)\t\t\t# find optimum delta\n\t\t\t{\n\t\t\tweight <- (m/alpha)*(1-pnorm(ey/2 + 1/ey*log(delta*m/(alpha*m1*prob))))\n\t\t\treturn(sum(weight))\n\t\t\t}\n\t\tweightSumVec <- sapply(delta,findDelta)\n\t\tdeltaOut <- delta[min(abs(weightSumVec-m))==abs(weightSumVec-m)]\t\t\t\t# find delta for which differenc is minimum\n\t\tWeight.out <- (m/alpha)*(1-pnorm(ey/2 + 1/ey*log(deltaOut*m/(alpha*m1*prob))))\t# approximate normalization\n\t\tnormWeight <- Weight.out/sum(Weight.out)*m\t\t\t\t\t\t\t# final normalization\n\t\treturn(normWeight)\n\t\t}\nrankProbsH01_weight <- fun.nomWeight.pro.bin(prob=rankProbsH01_norm,ey=ey,alpha=.05)\nrankProbsH01_weight_all <- rep(rankProbsH01_weight,each=grpSize)\nweight_rejection[k] <- sum(Ordered.pvalue2 <= alpha*rankProbsH01_weight_all/m)\nbon_rejection[k] <- sum(Ordered.pvalue2 <= alpha/m)\n}\nweight_rejection\nbon_rejection\n\n\n\n\n\n\n\n\n\n# continuous case3\n(using test stat instead of pvalue to use relation between filter effect and test effect)\n#============================================================================================\nm <- 100000\nnull=.5\nm0 <- m*null\nm1 <- m-m0\neffectVec <- seq(0,5,.5)\ni=5\nH <- rbinom(m,1,1-null)          \t   # alternative hypothesis true or false\nef <- effectVec[i]\ntest.ef <- rnorm(m,ef*H,1)\ntest.et <- rnorm(m,ef*H,1)\npval.ef <- 1-pnorm(test.ef)\npval.et <- 1-pnorm(test.et)\nData <- tibble(test.ef,pval.ef,test.et,pval.et)\t\t# better than data.frame()\nhist(pval.et,xlab=\"pvalue\",ylab=\"frequency\",main=\"distribution of pvalues\")\nordData <- Data[order(test.ef,decreasing=T),]\nOrdered.pvalue2 <- ordData[,4]\n\n\ngrp <- 100\ngrpSize <- 1000\npar(mfrow=c(3,3))\nranksProbInfo <- array(NA,c(20,100,2))\n#for(j in c(1,2,3,50,51,52,98,99,100))\t\t\t# need to make plots\nfor(j in 1:100)\n{\npvalPerGrp <- Ordered.pvalue2[(j*grpSize-grpSize+1):(j*grpSize)]\nh <- hist(pvalPerGrp,freq = FALSE,breaks=seq(0,1,length=11),xlab=paste(\"pvalue group or rank\",j))\n#h <- hist(pvalPerGrp,breaks=seq(0,1,length=21),plot=FALSE)\n#h$counts=h$counts/sum(h$counts)\n#plot(h)\nh$density=h$count/sum(h$count)\nxaxisValue <- h$mids\t\t\t\t# pvalue or quantile relates to effect size\nyaxisValue <- h$density/sum(h$density)\t# density or probability P(effect|rank)\nranksProbInfo[,j,1] <- xaxisValue\nranksProbInfo[,j,2] <- yaxisValue\n}\n\npar(mfrow=c(3,3))\nfor(i in 1:9)\n{\n#plot(1:100,ranksProbInfo[i,,1])\n#sum(ranksProbInfo[i,,1])\n\nplot(1:100,ranksProbInfo[i,,2],type=\"l\",lwd=2,xlab=\"ranks\",ylab=\"P(rank|effect)\")\nsum(ranksProbInfo[i,,2])\n}\n\n\n\n\n\nalpha=.05\npvalue <- c(0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95)\neffectsize <- qnorm(1-pvalue)\n\nweight_rejection <- c()\nbon_rejection <- c()\nfor(k in 1:10)\n{\ney <- effectsize[k]\nrankProbsH01 <- ranksProbInfo[k,,2]\nrankProbsH01_norm <- rankProbsH01/sum(rankProbsH01)\n\n# function to compute normalized weight for the binary case\n\t# prob=P(rank=k|filterEffect=ey), ey=filter efffect (effect of Y)\n\t#-----------------------------------------------------------------------------------------------------------\n\tfun.nomWeight.pro.bin <- function(prob,ey,alpha)\n\t\t{\t\t\t\t\t\t\t#input:p(rank|effect),filter effect; output: normalized weight\n\t\tm = length(prob)\n\t\tdelta <- seq(0,1,length=10000)\t\t# lagrange multiplier/ tuning parameter to obtain normalized wieght\n\t\tfindDelta <- function(delta)\t\t\t# find optimum delta\n\t\t\t{\n\t\t\tweight <- (m/alpha)*(1-pnorm(ey/2 + 1/ey*log(delta*m/(alpha*m1*prob))))\n\t\t\treturn(sum(weight))\n\t\t\t}\n\t\tweightSumVec <- sapply(delta,findDelta)\n\t\tdeltaOut <- delta[min(abs(weightSumVec-m))==abs(weightSumVec-m)]\t\t\t\t# find delta for which differenc is minimum\n\t\tWeight.out <- (m/alpha)*(1-pnorm(ey/2 + 1/ey*log(deltaOut*m/(alpha*m1*prob))))\t# approximate normalization\n\t\tnormWeight <- Weight.out/sum(Weight.out)*m\t\t\t\t\t\t\t# final normalization\n\t\treturn(normWeight)\n\t\t}\nrankProbsH01_weight <- fun.nomWeight.pro.bin(prob=rankProbsH01_norm,ey=ey,alpha=.05)\nrankProbsH01_weight_all <- rep(rankProbsH01_weight,each=grpSize)\nweight_rejection[k] <- sum(Ordered.pvalue2 <= alpha*rankProbsH01_weight_all/m)\nbon_rejection[k] <- sum(Ordered.pvalue2 <= alpha/m)\n}\nweight_rejection\nbon_rejection\n\n\n\n\n\n\n\n\n\n# continuous case3 (power calculation)\n#(using test stat instead of pvalue to use relation between filter effect and test effect)\n#============================================================================================\n\n# generate data this part will be unknown in practice\n#---------------------------------------------------\n\nfun.simu <- function(s,m=10000,null=.5,corr=0,random=0,alpha=.05,grp=100,grpSize=100,effectVec=effectVec)\n{\nfun.FwerPowerFdr <- function(i)\n\t{\n\tey <- effectVec[i]\n\tm0 <- ceiling(m*null)\n\tm1 <- m-m0\n\tSigma <- matrix(corr, 100, 100) + diag(100)*(1-corr)\t\t# test correlation matrix\n\txf <- rep(ey,m)\t\t\t\t\t\t\t\t# only alt. filter effect vector\n\txt <- if(random==0){rep(ey,m)} else {rnorm(m,ey,ey/2)} \t# alt. test effect vector\n\tH <- rbinom(m,1,1-null)          \t   # alternative hypothesis true or false\n\tef <- H*xf  \t\t\t\t# filter effect vector (mixture of null and alt)\n\tet <- H*xt\t\t\t\t\t# test effect vector (mixture of null and alt)\n\tmGrp = m/100\t\t\t\t# subgroup of tests.\n\n\t# function to generate test\n\t# input: r=no. of test groups,eVec=effect vector,Sigma=corr matrix\n\t# output: test = multivariate test statistics\n\t#-------------------------------------------------------------------\n\tfun.test <- function(r,eVec,Sigma)\n\t\t{\n\t\teSub <- eVec[(100*r+1-100):(100*r)]\n\t\ttest <- as.vector(rmvn(1,eSub,Sigma))\n\t\treturn(test)\n\t\t}\n\ttest.ef <- if(corr==0) {rnorm(m,ef,1)} else {as.vector(sapply(1:mGrp,fun.test,eVec=ef,Sigma=Sigma))}\t# filter test stat\n\ttest.et <- if(corr==0) {rnorm(m,et,1)} else {as.vector(sapply(1:mGrp,fun.test,eVec=et,Sigma=Sigma))}\t# actual test stat\n\tpval.ef <- pnorm(test.ef, lower.tail = FALSE)\t\t# filter test pvalues\n\tpval.et <- pnorm(test.et, lower.tail = FALSE)\t\t# actual test pvalues\n\n\t# this will be the data we will work on (known in practice)\n\t#-----------------------------------------------------------\n\tData <- data.frame(ef,tf=test.ef,pf=pval.ef,et,tt=test.et,pt=pval.et)\t# data of effect,test, and pvlaues\n\tOD <- Data[order(Data$tf,decreasing=T),]\n\tOrdered.pvalue <- OD$pt\n\n\t# this part will compute P(rank|effect) emperically\n\t# we will always use first plot because it will be the closest to the alternative probability\n\t#-------------------------------------------------------------------------------------------\n\tfun.ranksProb <- function(j)\n\t\t{\n\t\tpvalPerGrp <- Ordered.pvalue[(j*grpSize-grpSize+1):(j*grpSize)]\n\t\th <- hist(pvalPerGrp,freq = FALSE,breaks=seq(0,1,length=11))$density\n\t\tprob = h/sum(h)\n\t\treturn(prob)\n\t\t}\n\tranksProbMat <- sapply(1:grp,fun.ranksProb)\n\n\t#plotting data\n\t#-----------------------\n\tpar(mfrow=c(3,3))\n\tfor(k in 1:9)plot(1:100,ranksProbMat[k,],type=\"l\",lwd=2,xlab=\"ranks\",ylab=\"P(rank|effect)\")\n\n\t# need to estimate effect size to compute weight\n\t# since first plot is based on 25% quantile, need to find quantile for .05\n\t#---------------------------------------------------------------------------------\n\test.ef <- 3*mean(Data[,2])/sd(Data[,2])\n\n\trankProbsH01 <- ranksProbMat[1,]\n\trankProbsH01_smooth <- rankProbsH01/sum(rankProbsH01)\t\t\t# need to use spline-regression\n\trankProbsH01_final = rep(rankProbsH01_smooth,each=grpSize)\n\n\n\t# function to compute normalized weight for the binary case\n\t# prob=P(rank=k|filterEffect=ey), ey=filter efffect (effect of Y)\n\t#-----------------------------------------------------------------------------------------------------------\n\tfun.nomWeight.pro.bin <- function(prob,ey,alpha)\n\t\t{\n\t\tdelta <- seq(0,1,length=10000)\t\t# lagrange multiplier/ tuning parameter to obtain normalized wieght\n\t\tfindDelta <- function(delta)\t\t# find optimum delta\n\t\t\t{\n\t\t\tweight <- (m/alpha)*(1-pnorm(ey/2 + 1/ey*log(delta*m/(alpha*m1*prob))))\n\t\t\treturn(sum(weight,na.rm=T))\n\t\t\t}\n\t\tweightSumVec <- sapply(delta,findDelta)\n\t\tdeltaOut <- delta[min(abs(weightSumVec-m))==abs(weightSumVec-m)]\t\t# find delta for which difference is minimum\n\t\tdeltaOut <- ifelse(length(deltaOut)>1,.0001,deltaOut)\t\t\t\t# need adjustment for multiple delta\n\t\tW.out <- (m/alpha)*(1-pnorm(ey/2 + 1/ey*log(deltaOut*m/(alpha*m1*prob))))\t# approximate normalization\n\t\treturn(W.out)\n\t\t}\n\tW <- fun.nomWeight.pro.bin(prob=rankProbsH01_final,ey=abs(est.ef),alpha=alpha)\n\tweight_pro <- if(sum(W)==0){rep(1,m)} else {W/sum(W)*m}\t# normalizing proposed weight\n\n\t# pro=proposed,bon=bonferroni,rdw=roeder and wasserman,IHW=independent Hyp. Weight\n\t#----------------------------------------------------------------------------------------\n\tweight_rdw <- as.vector(RoederWasermanWeight(OD$tt,m=m,gamma=.05,alpha=alpha,rk=1000))\t\t# roeder wasserman weight\n\tihw_fwer <- ihw(OD$pt,OD$tf,alpha=alpha,adjustment_type = \"bonferroni\")\t\t# IHW method for FWER\n\tihw_fdr <-  ihw(OD$pt,OD$tf,alpha=alpha,adjustment_type = \"BH\")\t\t\t# IHW method for FDR\n\n\trej_pro <- OD$pt <= alpha*weight_pro/m\t\t\t# total rejections of all methods\n\trej_bon <- OD$pt <= alpha/m\n\trej_rdw <- OD$pt <= alpha*weight_rdw/m\n\trej_ihwFwer <- adj_pvalues(ihw_fwer) <= alpha\n\n\tFWER_pro <- sum(rej_pro[OD$et==0])/sum(OD$et==0)\t\t\t# FWER of proposed method\n\tFWER_bon <- sum(rej_bon[OD$et==0])/sum(OD$et==0)\t\t\t# FWER of bonferroni method\n\tFWER_rdw <- sum(rej_rdw[OD$et==0])/sum(OD$et==0)\t\t\t# FWER of Roeder Wasserman method\n\tFWER_ihw <- sum(rej_ihwFwer[OD$et==0])/sum(OD$et==0)\t\t\t# FWER of IHW method\n\n\tPOWER_pro <- sum(rej_pro[OD$et!=0])/max(1,sum(OD$et!=0))\t\t# power of proposed\n\tPOWER_bon <- sum(rej_bon[OD$et!=0])/max(1,sum(OD$et!=0))\t\t# power of bonferroni\n\tPOWER_rdw <- sum(rej_rdw[OD$et!=0])/max(1,sum(OD$et!=0))\t\t# power of Roeder Wasserman method\n\tPOWER_ihw <- sum(rej_ihwFwer[OD$et!=0])/max(1,sum(OD$et!=0))\t# power of IHW method\n\n\tadjPval_pro <- p.adjust(OD$pt/weight_pro, method=\"BH\")\t# adjusted pvalue to compute FDR\n\tadjPval_bon <- p.adjust(OD$pt, method=\"BH\")\n\tadjPval_rdw <- p.adjust(OD$pt/weight_rdw, method=\"BH\")\n\tadjPval_ihw <- adj_pvalues(ihw_fdr)\n\n\tFDR_pro <- sum(adjPval_pro[OD$et==0] <= alpha)/max(1,sum(adjPval_pro <= alpha))\t# FDR of proposed\n\tFDR_bh  <- sum(adjPval_bon[OD$et==0] <= alpha)/max(1,sum(adjPval_bon <= alpha))\t# FDR of benjaminin and hochberg\n\tFDR_rdw <- sum(adjPval_rdw[OD$et==0] <= alpha)/max(1,sum(adjPval_rdw <= alpha))\t# FDR of wasserman\n\tFDR_ihw <- sum(adjPval_ihw[OD$et==0] <= alpha)/max(1,rejections(ihw_fdr))\t\t# FDR of IHW method\n\n\treturn(c(FWER_pro,FWER_bon,FWER_rdw,FWER_ihw,POWER_pro,POWER_bon,POWER_rdw,POWER_ihw,FDR_pro,FDR_bh,FDR_rdw,FDR_ihw))\n\t}\nFwerPowerFdr <- sapply(1:length(effectVec),fun.FwerPowerFdr)\nreturn(FwerPowerFdr)\n}\n\neffectVec <- c(seq(0,1,.2),2,3,5,8)\nsimu=10\nFwerPowerFdr_list <- sapply(1:simu,fun.simu,m=10000,null=.5,corr=.9,random=0,alpha=.05,grp=100,grpSize=100,effectVec=effectVec,simplify = FALSE)\n\nFwerPowerFdr_list <- FwerPowerFdr_list1c\n\nFwerPowerFdr=0\nfor(i in 1:100)\n{\nFwerPowerFdr = FwerPowerFdr+FwerPowerFdr_list[[i]]\n}\nFwerPowerFdr=FwerPowerFdr/2\n\npar(mfrow=c(2,2))\nmatplot(effectVec,t(FwerPowerFdr[1:4,]),type=\"l\",lwd=2,main=\"FWER\")\nmatplot(effectVec,t(FwerPowerFdr[5:8,]),type=\"l\",lwd=2,main=\"Power\")\nmatplot(effectVec[1:6],t(FwerPowerFdr[5:8,1:6]),type=\"l\",lwd=2,main=\"Power\")\nmatplot(effectVec,t(FwerPowerFdr[9:12,]),type=\"l\",lwd=2,main=\"FDR\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# qvalue method\n#-----------------------------\nlibrary(qvalue)\nm <- 100000\nm0 <- m*.9\nm1 <- m-m0\nset.seed(123)\npval0 <- runif(m0,0,1)\nset.seed(123)\npval1 <-rbeta(m1,1,10)\npvalue <- sort(c(pval0,pval1))\n\n\ngrp <- 100\ngrpSize <- 1000\npi0 <- c()\npi1 <- c()\nfor(i in 1:grp)\n{\npval <- sample(pvalue[(i*grpSize-grpSize+1):(i*grpSize)])\npi0[i] <- qvalue(pval,lambda=seq(min(pval),max(pval),length=10))$pi0\n\npi1[i] <- 1 - pi0\n}\nmatplot(1:grp,cbind(pi0,pi1))\n\n\n\n\n\nsave.seed <- .Random.seed; set.seed(1)\nX   <- runif(20000, min=0, max=2.5)   # covariate\nH   <- rbinom(20000,1,0.1)            # hypothesis true or false\nZ   <- rnorm(20000, H*X)              # Z-score\n.Random.seed <- save.seed\npvalue <- 1-pnorm(Z)                  # pvalue\n\nihw_fdr <- ihw(pvalue, X, .1)        # Standard IHW for FDR control\nihw_fwer <- ihw(pvalue, X, .1, adjustment_type = \"bonferroni\")    # FWER control\ntable(H[adj_pvalues(ihw_fdr) <= 0.1] == 0) #how many false rejections?\ntable(H[adj_pvalues(ihw_fwer) <= 0.1] == 0)\n\n\npvalue <- 1 - pnorm(Z)                              # pvalue\nihw_res <- ihw(pvalue, covariates = X, alpha = 0.1)\nrejections(ihw_res)\ncolnames(as.data.frame(ihw_res))\n\n\n\n\n\n\n\n",
    "created" : 1496089654852.000,
    "dirty" : false,
    "encoding" : "ISO8859-1",
    "folds" : "",
    "hash" : "1185918020",
    "id" : "5E7FFA29",
    "lastKnownWriteTime" : 1488989160,
    "last_content_update" : 1488989160,
    "path" : "U:/Documents/My Research (UGA)/Multiple Hypoetheses/Result_Mohamad/Emperical methods.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}